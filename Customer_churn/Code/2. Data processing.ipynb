{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business problem:\n",
    "\n",
    "Companies usually have a greater focus on customer acquisition than customer. However, it can cost anywhere between five to twenty five times more to attract a new customer than retain an existing one. Increasing customer retention rates by 5% can increase profits by 25%, according to a research done by Bain & Company.\n",
    "\n",
    "Churn is a metric that measures the no. of customers who stop doing business with a company. Through this metric, most businesses would try to understand the reason behind churn numbers and tackle those factors with reactive action plans.\n",
    "\n",
    "But what if you could identify a customer who is likely to churn and take appropriate steps to prevent it from happening? The reasons that lead customers to the cancellation decision can be numerous, ranging from poor service quality to new competitors entering the market. Usually, there is no single reason, but a combination of factors that result to customer churn.\n",
    "\n",
    "Although the customers have churned, their data is still available. Through machine learning we can sift through this valuable data to discover patterns and understand the combination of different factors which lead to customer churn.\n",
    "\n",
    "Our goal in this project is to identify behavior among customers who are likely to churn. Subsequent to that we need to train a machine learning model to identify these signals from a customer before they churn. Once deployed, our model will identify customers who might churn and alert us to take necessary steps to prevent their churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\r\n",
    "#\r\n",
    "#Importing libraries\r\n",
    "#\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelBinarizer\r\n",
    "from sklearn.model_selection import train_test_split as tts\r\n",
    "from sklearn.naive_bayes import BernoulliNB\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "from numpy import random\r\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, recall_score as R\r\n",
    "import warnings\r\n",
    "import pickle\r\n",
    "###############################################################################\r\n",
    "#\r\n",
    "#Notebook options\r\n",
    "#\r\n",
    "pd.options.display.max_columns =100\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "###############################################################################\r\n",
    "#\r\n",
    "#Reading the data\r\n",
    "#\r\n",
    "df = pd.read_csv(r\"../Data/Telco-Customer-Churn.csv\")\r\n",
    "df.drop([\"customerID\"], axis=1, inplace=True)\r\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#\n",
    "# def preprocess(df)\n",
    "# Input:\n",
    "# df = input dataframe \n",
    "#\n",
    "# 1. Prepare X and y as feature and target matrix\n",
    "# 2. Binarize the target feature y\n",
    "# 3. Segregate columns into binary, numeric and categorical features\n",
    "# 4. Binarize the binary features\n",
    "# 5. Convert categorical features to dummy variables\n",
    "# 6. Perform standard scaling for all features\n",
    "# 7. Convert the numpy arrays to dataframes for furthur processing\n",
    "# 8. Return the formatted data\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "  df.TotalCharges.astype('float')\n",
    "  X=df.drop('Churn', axis=1)\n",
    "  y=df.Churn\n",
    "\n",
    "  lb=LabelBinarizer()\n",
    "  y=lb.fit_transform(y)\n",
    "\n",
    "  binary_feat = X.nunique()[X.nunique() == 2].keys().tolist()\n",
    "  numeric_feat = [col for col in X.select_dtypes(['float','int']).columns.tolist() if col not in binary_feat]\n",
    "  categorical_feat = [ col for col in X.select_dtypes('object').columns.to_list() if col not in binary_feat + numeric_feat ]\n",
    "\n",
    "  #le = LabelEncoder()\n",
    "  for i in binary_feat:\n",
    "    X[i] = lb.fit_transform(X[i])\n",
    "\n",
    "  X = pd.get_dummies(X, columns=categorical_feat)\n",
    "  sc=StandardScaler()\n",
    "  cols=X.columns\n",
    "  x = sc.fit_transform(X[['TotalCharges','MonthlyCharges','tenure']])\n",
    "\n",
    "  X=pd.concat([X,pd.DataFrame(x)],axis=1)\n",
    "  X.drop(['tenure','MonthlyCharges','TotalCharges'],axis=1,inplace=True)\n",
    "  X=pd.DataFrame(X)\n",
    "  y=pd.DataFrame(y)\n",
    "\n",
    "  with open('../deployment/scalar','wb') as a:\n",
    "    pickle.dump(sc,a)\n",
    "\n",
    "  return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling the over represented class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#\n",
    "# def undersample(X,y)\n",
    "# Input\n",
    "# X = feature vectors\n",
    "# y = response vector\n",
    "#\n",
    "# 1. Split the data into test and train sets\n",
    "# 2. Undersample the overrepresented class in the training set\n",
    "# 3. Perform standard scaling on the new set of features because the original distribution has been undersampled\n",
    "# 4. Apply the same transformation on the test feature set\n",
    "# 5. Return the X_train, X_test, y_train, y_test\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def undersample(X,y):\n",
    "    x_train, X_test, y_tr,y_test = tts(X,y, test_size=0.2,random_state=123456)\n",
    "    rus = RandomUnderSampler()\n",
    "    X_train, y_train = rus.fit_resample(x_train, y_tr)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62a490a61034db1e8b6e05e6e999b5629625384177f9d8186b44b788547d5428"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('machine_L': venv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "25e55770fcf34c599020a25d496c9f3b95f72a4b8e22976c31f9a97ef8dc14b4"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}