{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business problem:\n",
    "\n",
    "Companies usually have a greater focus on customer acquisition than customer. However, it can cost anywhere between five to twenty five times more to attract a new customer than retain an existing one. Increasing customer retention rates by 5% can increase profits by 25%, according to a research done by Bain & Company.\n",
    "\n",
    "Churn is a metric that measures the no. of customers who stop doing business with a company. Through this metric, most businesses would try to understand the reason behind churn numbers and tackle those factors with reactive action plans.\n",
    "\n",
    "But what if you could identify a customer who is likely to churn and take appropriate steps to prevent it from happening? The reasons that lead customers to the cancellation decision can be numerous, ranging from poor service quality to new competitors entering the market. Usually, there is no single reason, but a combination of factors that result to customer churn.\n",
    "\n",
    "Although the customers have churned, their data is still available. Through machine learning we can sift through this valuable data to discover patterns and understand the combination of different factors which lead to customer churn.\n",
    "\n",
    "Our goal in this project is to identify behavior among customers who are likely to churn. Subsequent to that we need to train a machine learning model to identify these signals from a customer before they churn. Once deployed, our model will identify customers who might churn and alert us to take necessary steps to prevent their churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take input as formatted data from the data processed python file. \r\n",
    "\r\n",
    "# Use HyperOpt and genetic algorithms for parameter tuning\r\n",
    "\r\n",
    "# Output the best model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\r\n",
    "#\r\n",
    "#Importing libraries\r\n",
    "#\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelBinarizer\r\n",
    "from sklearn.model_selection import train_test_split as tts\r\n",
    "from sklearn.naive_bayes import BernoulliNB\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "from numpy import random\r\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, recall_score as R\r\n",
    "import warnings\r\n",
    "import pickle\r\n",
    "###############################################################################\r\n",
    "#\r\n",
    "#Notebook options\r\n",
    "#\r\n",
    "pd.options.display.max_columns =100\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "###############################################################################\r\n",
    "#\r\n",
    "#Reading the data\r\n",
    "#\r\n",
    "df = pd.read_csv(r\"../Data/Telco-Customer-Churn.csv\")\r\n",
    "df.drop([\"customerID\"], axis=1, inplace=True)\r\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#\n",
    "# def preprocess(df)\n",
    "# Input:\n",
    "# df = input dataframe \n",
    "#\n",
    "# 1. Prepare X and y as feature and target matrix\n",
    "# 2. Binarize the target feature y\n",
    "# 3. Segregate columns into binary, numeric and categorical features\n",
    "# 4. Binarize the binary features\n",
    "# 5. Convert categorical features to dummy variables\n",
    "# 6. Perform standard scaling for all features\n",
    "# 7. Convert the numpy arrays to dataframes for furthur processing\n",
    "# 8. Return the formatted data\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "  df.TotalCharges.astype('float')\n",
    "  X=df.drop('Churn', axis=1)\n",
    "  y=df.Churn\n",
    "\n",
    "  lb=LabelBinarizer()\n",
    "  y=lb.fit_transform(y)\n",
    "\n",
    "  binary_feat = X.nunique()[X.nunique() == 2].keys().tolist()\n",
    "  numeric_feat = [col for col in X.select_dtypes(['float','int']).columns.tolist() if col not in binary_feat]\n",
    "  categorical_feat = [ col for col in X.select_dtypes('object').columns.to_list() if col not in binary_feat + numeric_feat ]\n",
    "\n",
    "  #le = LabelEncoder()\n",
    "  for i in binary_feat:\n",
    "    X[i] = lb.fit_transform(X[i])\n",
    "\n",
    "  X = pd.get_dummies(X, columns=categorical_feat)\n",
    "  sc=StandardScaler()\n",
    "  cols=X.columns\n",
    "  x = sc.fit_transform(X[['TotalCharges','MonthlyCharges','tenure']])\n",
    "\n",
    "  X=pd.concat([X,pd.DataFrame(x)],axis=1)\n",
    "  X.drop(['tenure','MonthlyCharges','TotalCharges'],axis=1,inplace=True)\n",
    "  X=pd.DataFrame(X)\n",
    "  y=pd.DataFrame(y)\n",
    "\n",
    "  with open('../deployment/scalar','wb') as a:\n",
    "    pickle.dump(sc,a)\n",
    "\n",
    "  return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling the over represented class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#\n",
    "# def undersample(X,y)\n",
    "# Input\n",
    "# X = feature vectors\n",
    "# y = response vector\n",
    "#\n",
    "# 1. Split the data into test and train sets\n",
    "# 2. Undersample the overrepresented class in the training set\n",
    "# 3. Perform standard scaling on the new set of features because the original distribution has been undersampled\n",
    "# 4. Apply the same transformation on the test feature set\n",
    "# 5. Return the X_train, X_test, y_train, y_test\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def undersample(X,y):\n",
    "    x_train, X_test, y_tr,y_test = tts(X,y, test_size=0.2,random_state=123456)\n",
    "    rus = RandomUnderSampler()\n",
    "    X_train, y_train = rus.fit_resample(x_train, y_tr)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#\n",
    "# def build(model,X_train, X_test, y_train, y_test,c,e)\n",
    "# \n",
    "# Input:\n",
    "# model = the model object\n",
    "# X_train, X_test = standard scaled train and test features\n",
    "# y_train, y_test = response variables in train and test set\n",
    "# c = Revenue lost due to churn of a singular customer\n",
    "# e = Cost of focusing effort on a single to prevent his/her churn\n",
    "#\n",
    "# 1. Initialise variables \n",
    "# 2. Fit the model\n",
    "# 3. r = recall_score\n",
    "# 4. p = count of missed positive churn predictions \n",
    "# 5. Unravel the confusion matrix\n",
    "# 6. Calculate F1 score\n",
    "# 7. Calculate ROC score\n",
    "# 8. Revenue = (Predicted & actual churn)*c -(count of missed actual positive prediction)*c - (predicted churn)*e\n",
    "# 9. Send computed values to business function\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def build(model,X_train, X_test, y_train, y_test,c,e):  \n",
    "\n",
    "    rev,p,r=0,0,0\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pr=model.predict(X_test)\n",
    "\n",
    "    r=round(R(y_test,y_pr)*100,2)\n",
    "    p=round((100-r),2)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pr).ravel()\n",
    "    f1 = f1_score(y_test,y_pr)\n",
    "    roc = roc_auc_score(y_test, y_pr)\n",
    "    rev = tp*c -fn*c - (tp+fp)*e\n",
    "    return round(r,2), round(p,2), f1, roc,rev  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#\n",
    "# def compute(df,c,e):\n",
    "# \n",
    "# Input:\n",
    "# df = input dataframe  \n",
    "# c = Revenue lost due to churn of a singular customer\n",
    "# e = Cost of focusing effort on a single to prevent his/her chur\n",
    "#\n",
    "# 1. Preprocesses the data\n",
    "# 2. Splits the data into training and test set; undersamples the overrepresented class in training set \n",
    "# 3. Prepares models and calculates parameters\n",
    "# 4. Collates all the data into a mod dataframe\n",
    "# 5. Return the comparison database\n",
    "#\n",
    "#########################################################################################\n",
    "\n",
    "def compute(df, *var):\n",
    "    \n",
    "    c=var[0]\n",
    "    e=var[1]\n",
    "    X,y=preprocess(df)\n",
    "    X_train, X_test, y_train, y_test = undersample(X,y)\n",
    "\n",
    "    svm = SVC(kernel='rbf')\n",
    "    b=BernoulliNB(alpha=0.6)\n",
    "    gbc=GradientBoostingClassifier(learning_rate=0.2, max_depth=1, n_estimators=150)\n",
    "    ada=AdaBoostClassifier(learning_rate=0.3, n_estimators=70)\n",
    "    lr=LogisticRegression(C=0.001)\n",
    "    rf=RandomForestClassifier(max_depth=10,n_estimators=120, class_weight=\"balanced_subsample\", random_state=123456)   \n",
    "\n",
    "    pred_b,misd_b,f1_b,roc_b,rev_b=build(b,X_train, X_test, y_train, y_test,c,e)\n",
    "    pred_knn,misd_knn,f1_knn,roc_knn,rev_knn=build(gbc,X_train, X_test, y_train, y_test,c,e)\n",
    "    pred_ada,misd_ada,f1_ada,roc_ada,rev_ada=build(ada,X_train, X_test, y_train, y_test,c,e)\n",
    "    pred_lr,misd_lr,f1_lr,roc_lr,rev_lr=build(lr,X_train, X_test, y_train, y_test,c,e)\n",
    "    pred_svm,misd_svm,f1_svm,roc_svm,rev_svm=build(svm,X_train, X_test, y_train, y_test,c,e)\n",
    "    pred_rf,misd_rf,f1_rf,roc_rf,rev_rf=build(rf,X_train, X_test, y_train, y_test,c,e)\n",
    "\n",
    "    rev = [rev_svm,rev_b,rev_knn,rev_lr,rev_rf,rev_ada]\n",
    "    misd= [misd_svm,misd_b,misd_knn,misd_lr,misd_rf,misd_ada]\n",
    "    pred= [pred_svm,pred_b,pred_knn,pred_lr,pred_rf,pred_ada]\n",
    "    f1s= [f1_svm,f1_b,f1_knn,f1_lr,f1_rf,f1_ada]\n",
    "    rocs= [roc_svm,roc_b,roc_knn,roc_lr,roc_rf,roc_ada]\n",
    "    mod = [\"Support Vector Machine\", \"Naive Bayes\",\"Gradient Boosting Classifier\",\"Logistic regression\",\"Random Forest\",\"AdaBoost Classifier\"]\n",
    "    model = [svm,b,gbc,lr,rf,ada]\n",
    "\n",
    "    mod = pd.DataFrame({\"Revenue saved\":rev,\"Predicted(True positive)\":pred,\"Missed(False negative)\":misd,\"F1 score\":f1s, \"ROC_AUC\":rocs,\"Model\":model}, index=mod)\n",
    "    mod.sort_values([\"Revenue saved\"], ascending=False,inplace=True)\n",
    "    \n",
    "    return mod,X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business implication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#\n",
    "# def business(mod,c,e):\n",
    "# Input:\n",
    "# mod = model details dataframe\n",
    "# c = Revenue lost due to churn of a singular customer\n",
    "# e = Cost of focusing effort on a single to prevent his/her chur\n",
    "#  \n",
    "# 1. lost = Total revenue lost due to churn = (Total reponses=1, i.e. churn) * (cost of churn)\n",
    "# 2. best = best model as per revenue\n",
    "# 3. Initialize some variables\n",
    "# 4. Draw >30 samples of size 50% of df\n",
    "# 5. x = Selecting random set of 50% customers\n",
    "# 6. saved_churn = Count of churn predicted by the current sample * cost of churn\n",
    "# 7. cost_of_effort = Total expenditure by focusing effort on random 50% of df\n",
    "# 8. money_that_could_have_been_saved = difference between focusing effort on random 50% of the population and return on it by saving churn\n",
    "# 9. cost_saved_by_model = revenue loss prevented by predicting churn using our model\n",
    "# 10. avg_rev_lost = averages the money that could have been saved over all the 50 samples drawn each time\n",
    "# 11. avg_money_saved = cost saved by our model + money that could have been saved\n",
    "# 12. cost = averages the money that could have been saved over all sample draws\n",
    "# 13. gained_rev = averages the money saved by our model as compared to random sampling over all sample draws\n",
    "# 14. Print summary\n",
    "#\n",
    "#########################################################################################\n",
    "\n",
    "def business(df,y,mod,c,e):\n",
    "\n",
    "    lb=LabelBinarizer()\n",
    "    lost=lb.fit_transform(y).sum()*c \n",
    "\n",
    "    best=mod.head(1)\n",
    "\n",
    "    avg_rev_lost=[]\n",
    "    avg_money_saved=[]\n",
    "\n",
    "    for i in range(1,31):\n",
    "        var_churn=0.5\n",
    "        x = random.randint(df.shape[0], size=(round(df.shape[0]*var_churn)))     \n",
    "        saved_churn = (lb.fit_transform(df.Churn.iloc[x])).sum()*c  \n",
    "        cost_of_effort = x.shape[0]*e\n",
    "        \n",
    "        money_that_could_have_been_saved = cost_of_effort-saved_churn\n",
    "        cost_saved_by_model=best.iloc[0,0]\n",
    "\n",
    "        avg_rev_lost.append(money_that_could_have_been_saved)\n",
    "        avg_money_saved.append(cost_saved_by_model+money_that_could_have_been_saved)\n",
    "\n",
    "    cost = pd.DataFrame(avg_rev_lost).mean()\n",
    "    gained_rev = pd.DataFrame(avg_money_saved).mean()\n",
    "\n",
    "    print(f\"Lost revenue if we do not prevent churn = Rs.{lost} \\n\") \n",
    "    print(f\"Assumed cost of losing a customer: Rs.{c} \\nAssumed cost of effort to prevent churn: Rs.{e} \\n\")\n",
    "    print(f\"Percentage of customers predicted by '{best.index[0]}' who were going to churn: {best.iloc[0,1]}%\")\n",
    "    print(f\"Percentage of customers missed by '{best.index[0]}' who were going to churn: {best.iloc[0,2]}%\")\n",
    "    print(\"Revenue saved by preventing churn with our model as compared to no model = Rs\", best.iloc[0,0])\n",
    "    print(f\"\\n\\nTotal expenditure for preventing churn on random {var_churn*100}% of customers: Rs.{cost_of_effort}\")\n",
    "    print(f\"Extra cost to prevent churn within random {var_churn*100}% of the customers = Rs.{round(cost[0])}\")\n",
    "    print(f\"Our '{best.index[0]}' model saves us Rs.{round(gained_rev[0])} on an average compared to a random selection of 50% customers\")\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost revenue if we do not prevent churn = Rs.9345000 \n",
      "\n",
      "Assumed cost of losing a customer: Rs.5000 \n",
      "Assumed cost of effort to prevent churn: Rs.1500 \n",
      "\n",
      "Percentage of customers predicted by 'Random Forest' who were going to churn: 84.37%\n",
      "Percentage of customers missed by 'Random Forest' who were going to churn: 15.63%\n",
      "Revenue saved by preventing churn with our model as compared to no model = Rs 272500\n",
      "\n",
      "\n",
      "Total expenditure for preventing churn on random 50.0% of customers: Rs.5283000\n",
      "Extra cost to prevent churn within random 50.0% of the customers = Rs.626833\n",
      "Our 'Random Forest' model saves us Rs.899333 on an average compared to a random selection of 50% customers\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    c = input(\"Enter the revenue lost due to churn of a single customer\")\n",
    "    e = input(\"Cost of focusing effort on a customer to prevent his/her churn\")\n",
    "    c = 0 if c=='' else int(c)\n",
    "    e = 0 if e=='' else int(e)\n",
    "    if c>0 and e>0:\n",
    "        var = [c,e]\n",
    "    else: \n",
    "        var=[5000,1500]\n",
    "    mod,X,y=compute(df,*var)\n",
    "    business(df,y,mod,*var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue saved</th>\n",
       "      <th>Predicted(True positive)</th>\n",
       "      <th>Missed(False negative)</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>272500</td>\n",
       "      <td>84.37</td>\n",
       "      <td>15.63</td>\n",
       "      <td>0.612420</td>\n",
       "      <td>0.777436</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=10, max_feat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression</th>\n",
       "      <td>226500</td>\n",
       "      <td>83.19</td>\n",
       "      <td>16.81</td>\n",
       "      <td>0.601279</td>\n",
       "      <td>0.767798</td>\n",
       "      <td>LogisticRegression(C=0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting Classifier</th>\n",
       "      <td>217000</td>\n",
       "      <td>82.60</td>\n",
       "      <td>17.40</td>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.767185</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost Classifier</th>\n",
       "      <td>209000</td>\n",
       "      <td>82.89</td>\n",
       "      <td>17.11</td>\n",
       "      <td>0.595970</td>\n",
       "      <td>0.763520</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>206000</td>\n",
       "      <td>86.43</td>\n",
       "      <td>13.57</td>\n",
       "      <td>0.571707</td>\n",
       "      <td>0.748509</td>\n",
       "      <td>BernoulliNB(alpha=0.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>182500</td>\n",
       "      <td>81.71</td>\n",
       "      <td>18.29</td>\n",
       "      <td>0.593148</td>\n",
       "      <td>0.759956</td>\n",
       "      <td>SVC()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Revenue saved  Predicted(True positive)  \\\n",
       "Random Forest                        272500                     84.37   \n",
       "Logistic regression                  226500                     83.19   \n",
       "Gradient Boosting Classifier         217000                     82.60   \n",
       "AdaBoost Classifier                  209000                     82.89   \n",
       "Naive Bayes                          206000                     86.43   \n",
       "Support Vector Machine               182500                     81.71   \n",
       "\n",
       "                              Missed(False negative)  F1 score   ROC_AUC  \\\n",
       "Random Forest                                  15.63  0.612420  0.777436   \n",
       "Logistic regression                            16.81  0.601279  0.767798   \n",
       "Gradient Boosting Classifier                   17.40  0.601504  0.767185   \n",
       "AdaBoost Classifier                            17.11  0.595970  0.763520   \n",
       "Naive Bayes                                    13.57  0.571707  0.748509   \n",
       "Support Vector Machine                         18.29  0.593148  0.759956   \n",
       "\n",
       "                                                                          Model  \n",
       "Random Forest                 (DecisionTreeClassifier(max_depth=10, max_feat...  \n",
       "Logistic regression                                 LogisticRegression(C=0.001)  \n",
       "Gradient Boosting Classifier  ([DecisionTreeRegressor(criterion='friedman_ms...  \n",
       "AdaBoost Classifier           (DecisionTreeClassifier(max_depth=1, random_st...  \n",
       "Naive Bayes                                              BernoulliNB(alpha=0.6)  \n",
       "Support Vector Machine                                                    SVC()  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced_subsample', max_depth=10,\n",
       "                       n_estimators=120, random_state=123456)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=RandomForestClassifier(max_depth=10,n_estimators=120, class_weight=\"balanced_subsample\", random_state=123456)\r\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../deployment/model','wb') as a:\n",
    "    pickle.dump(model,a)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62a490a61034db1e8b6e05e6e999b5629625384177f9d8186b44b788547d5428"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('machine_L': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "25e55770fcf34c599020a25d496c9f3b95f72a4b8e22976c31f9a97ef8dc14b4"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}